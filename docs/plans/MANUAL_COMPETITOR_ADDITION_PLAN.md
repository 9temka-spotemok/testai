# План реализации: Ручное добавление конкурентов

## Обзор

Реализация функционала для ручного добавления компаний-конкурентов через веб-интерфейс с автоматическим сканированием их новостных ресурсов.

## Цели

1. Позволить пользователям добавлять компании вручную, если они не найдены в списке
2. Автоматически сканировать новостные ресурсы компании (blog, news и т.д.)
3. Извлекать отдельные новостные статьи с их страниц
4. Предоставлять превью перед добавлением
5. Обрабатывать дубликаты - дополнять информацию существующих компаний

## Требования

### Функциональные требования

1. **Аутентификация**: Только авторизованные пользователи могут добавлять компании
2. **Проверка дубликатов**: 
   - Сравнение по URL (нормализованному) и названию
   - Если компания существует - дополнять информацию, а не создавать дубликат
   - Сохранять новые источники новостей для улучшения алгоритма поиска
3. **Сканирование**:
   - Автоматическое определение страницы с новостями (blog, news, insights и т.д.)
   - Возможность ручного указания URL страницы с новостями
   - Извлечение отдельных статей (например: `/blog/article-name/`)
   - Ограничение: до 50 статей для быстрого превью (синхронное сканирование)
4. **Превью результатов**:
   - Информация о компании (название, описание, логотип, категория)
   - Статистика новостей (количество, категории, типы источников)
   - Примеры найденных статей

### Технические требования

1. **Backend**:
   - Синхронное сканирование (таймаут 30 секунд)
   - Использование существующего `UniversalBlogScraper`
   - Новые эндпоинты в `/api/v1/companies/`
   - Извлечение метаданных компании (название, описание, логотип)

2. **Frontend**:
   - Модальное окно с пошаговым процессом
   - Интеграция в Dashboard → Competitors list
   - Обработка ошибок и состояний загрузки

## Архитектура решения

### Backend компоненты

#### 1. Модификация UniversalBlogScraper
**Файл**: `backend/app/scrapers/universal_scraper.py`

**Изменения**:
- Добавить параметр `news_page_url: Optional[str]` в метод `scrape_company_blog()`
- Если `news_page_url` указан - использовать его вместо автоопределения
- Улучшить извлечение отдельных статей со страницы блога

**Методы для модификации**:
- `scrape_company_blog()` - добавить поддержку ручного URL

#### 2. Новый сервис: CompanyInfoExtractor
**Файл**: `backend/app/services/company_info_extractor.py` (новый)

**Функционал**:
- Извлечение названия компании из `<title>` или `og:title`
- Извлечение описания из `meta description` или `og:description`
- Извлечение логотипа из favicon, `og:image` или селекторов логотипа
- Определение категории по домену/названию (эвристика)

**Методы**:
- `extract_company_info(website_url: str) -> Dict[str, Optional[str]]`

#### 3. Новые эндпоинты в Companies API
**Файл**: `backend/app/api/v1/endpoints/companies.py`

**Новые эндпоинты**:

1. **POST `/api/v1/companies/scan`**
   - Синхронное сканирование компании
   - Вход: `{ website_url: str, news_page_url?: str }`
   - Выход: превью компании и новостей
   - Таймаут: 30 секунд
   - Лимит: до 50 статей для быстрого превью

2. **POST `/api/v1/companies/`**
   - Создание или обновление компании
   - Вход: `{ company: {...}, news_items: [...] }`
   - Логика:
     - Проверка дубликатов по нормализованному URL и названию
     - Если существует - дополнение информации
     - Если нет - создание новой компании
     - Сохранение новостей (пропуск дубликатов по `source_url`)

**Вспомогательные функции**:
- `normalize_url(url: str) -> str` - нормализация URL для сравнения

#### 4. Обновление моделей (если нужно)
**Файл**: `backend/app/models/company.py`

**Проверка**: Убедиться, что все необходимые поля есть в модели `Company`

### Frontend компоненты

#### 1. Новый компонент: AddCompetitorModal
**Файл**: `frontend/src/components/AddCompetitorModal.tsx` (новый)

**Функционал**:
- Модальное окно с шагами:
  1. **Input**: Ввод URL компании и опционально URL страницы с новостями
  2. **Scanning**: Индикатор загрузки во время сканирования
  3. **Preview**: Превью результатов (компания, статистика, примеры статей)
  4. **Confirming**: Подтверждение и создание

**Состояния**:
- `step: 'input' | 'scanning' | 'preview' | 'confirming'`
- Обработка ошибок (если страница не найдена - показать поле для ручного URL)

#### 2. Обновление типов
**Файл**: `frontend/src/types/index.ts`

**Новые типы**:
- `CompanyScanRequest`
- `CompanyScanResult`
- `CreateCompanyRequest`
- `CreateCompanyResponse`

#### 3. Обновление API Service
**Файл**: `frontend/src/services/api.ts`

**Новые методы**:
- `scanCompany(request: CompanyScanRequest): Promise<CompanyScanResult>`
- `createCompany(request: CreateCompanyRequest): Promise<CreateCompanyResponse>`

#### 4. Интеграция в DashboardPage
**Файл**: `frontend/src/pages/DashboardPage.tsx`

**Изменения**:
- Добавить кнопку "Add Competitor" на вкладке Competitors list
- Импортировать и использовать `AddCompetitorModal`
- Обновлять список компаний после успешного добавления

## Детальный план реализации

### Этап 1: Backend - Модификация Scraper (30 мин)

1. ✅ Обновить `UniversalBlogScraper.scrape_company_blog()`
   - Добавить параметр `news_page_url: Optional[str]`
   - Использовать ручной URL если указан
   - Улучшить логику извлечения статей

### Этап 2: Backend - Company Info Extractor (45 мин)

1. ✅ Создать `backend/app/services/company_info_extractor.py`
2. ✅ Реализовать `extract_company_info()`
   - Парсинг HTML страницы
   - Извлечение метаданных
   - Определение категории

### Этап 3: Backend - API Endpoints (1.5 часа)

1. ✅ Добавить `normalize_url()` функцию в `companies.py`
2. ✅ Реализовать `POST /companies/scan`
   - Валидация URL
   - Вызов `extract_company_info()`
   - Вызов `scraper.scrape_company_blog()`
   - Формирование ответа с превью
3. ✅ Реализовать `POST /companies/`
   - Проверка дубликатов
   - Логика создания/обновления
   - Сохранение новостей

### Этап 4: Frontend - Типы и API (30 мин)

1. ✅ Добавить типы в `types/index.ts`
2. ✅ Добавить методы в `ApiService`

### Этап 5: Frontend - Компонент Modal (2 часа)

1. ✅ Создать `AddCompetitorModal.tsx`
2. ✅ Реализовать шаги:
   - Input форма
   - Scanning состояние
   - Preview результатов
   - Confirming состояние
3. ✅ Обработка ошибок
4. ✅ Валидация форм

### Этап 6: Frontend - Интеграция (30 мин)

1. ✅ Добавить кнопку в `DashboardPage.tsx`
2. ✅ Интегрировать модальное окно
3. ✅ Обновление списка после добавления

### Этап 7: Тестирование (1 час)

1. ✅ Тестирование сканирования различных сайтов
2. ✅ Тестирование обработки дубликатов
3. ✅ Тестирование UI/UX
4. ✅ Обработка edge cases

### Этап 8: Документация (30 мин)

1. ✅ Обновить README.md
2. ✅ Добавить комментарии в код
3. ✅ Обновить API документацию (если есть)

## TODO: Будущие улучшения

### Асинхронное сканирование для больших сайтов

**Проблема**: Синхронное сканирование ограничено 30 секундами и 50 статьями

**Решение**:
1. Добавить Celery задачу `scan_company_async`
2. Эндпоинт `POST /companies/scan/async` - возвращает `task_id`
3. Эндпоинт `GET /companies/scan/status/{task_id}` - проверка статуса
4. Frontend: polling статуса или WebSocket для real-time обновлений

**Файлы для изменения**:
- `backend/app/tasks/scraping.py` - добавить задачу
- `backend/app/api/v1/endpoints/companies.py` - добавить эндпоинты
- `frontend/src/components/AddCompetitorModal.tsx` - добавить async режим

### Кэширование результатов сканирования

**Проблема**: Повторное сканирование одного и того же URL занимает время

**Решение**:
- Кэшировать результаты сканирования на 1 час
- Использовать Redis или in-memory cache
- Ключ: нормализованный URL

### Улучшенное извлечение метаданных

**Проблема**: Базовое извлечение может не найти всю информацию

**Решение**:
- Использовать AI для извлечения описания компании
- Использовать favicon API для лучшего логотипа
- Использовать внешние API для определения категории

### Валидация и санитизация данных

**Проблема**: Пользователь может ввести некорректные данные

**Решение**:
- Строгая валидация URL
- Проверка доступности сайта перед сканированием
- Санитизация HTML контента
- Валидация названия компании (длина, символы)

### Rate Limiting

**Проблема**: Пользователи могут злоупотреблять сканированием

**Решение**:
- Ограничение: 10 сканирований в час на пользователя
- Использование Redis для отслеживания лимитов

## Обработка ошибок

### Backend ошибки

1. **Невалидный URL**: HTTP 400 с сообщением
2. **Сайт недоступен**: HTTP 404 или 500 с понятным сообщением
3. **Новости не найдены**: Возвращать пустой результат, предлагать ручной URL
4. **Таймаут**: HTTP 504 с предложением использовать async версию (в будущем)

### Frontend ошибки

1. **Ошибка сканирования**: Показать сообщение, предложить ручной URL
2. **Ошибка создания**: Показать сообщение, вернуться к preview
3. **Сетевые ошибки**: Показать общее сообщение, предложить повторить

## Примеры использования

### Пример 1: Автоматическое определение страницы с новостями

```
Пользователь вводит: https://www.accuranker.com
Система автоматически находит: https://www.accuranker.com/blog/
Сканирует и находит статьи:
  - https://www.accuranker.com/blog/what-are-prompts/
  - https://www.accuranker.com/blog/track-your-llm-performance-with-accullm/
  - и т.д.
```

### Пример 2: Ручное указание страницы с новостями

```
Пользователь вводит: https://example.com
Автоматическое определение не находит страницу с новостями
Пользователь указывает вручную: https://example.com/news/
Система сканирует указанную страницу
```

### Пример 3: Обновление существующей компании

```
Пользователь вводит: https://existing-company.com
Система находит существующую компанию по URL
Дополняет информацию:
  - Добавляет описание (если отсутствовало)
  - Добавляет логотип (если отсутствовал)
  - Добавляет новые источники новостей
  - Сохраняет новые статьи
```

## Критерии приемки

1. ✅ Пользователь может добавить компанию через модальное окно
2. ✅ Система автоматически определяет страницу с новостями
3. ✅ Если автоматическое определение не работает - можно указать URL вручную
4. ✅ Система извлекает отдельные статьи со страницы
5. ✅ Показывается превью перед добавлением
6. ✅ Если компания существует - информация дополняется, а не создается дубликат
7. ✅ Новости сохраняются и отображаются в системе
8. ✅ Обработка ошибок работает корректно
9. ✅ UI/UX интуитивен и понятен

## Риски и митигация

### Риск 1: Сайт блокирует сканирование
**Митигация**: Использовать правильные User-Agent заголовки, добавить задержки между запросами

### Риск 2: Нестандартная структура страницы
**Митигация**: Множественные селекторы для извлечения статей, fallback методы

### Риск 3: Таймаут при большом количестве статей
**Митигация**: Ограничение до 50 статей для синхронного сканирования, TODO для async версии

### Риск 4: Дубликаты компаний
**Митигация**: Нормализация URL, проверка по URL и названию, логика дополнения вместо создания

## Зависимости

### Backend
- Существующий `UniversalBlogScraper` ✅
- `httpx` для HTTP запросов ✅
- `BeautifulSoup` для парсинга HTML ✅
- FastAPI для эндпоинтов ✅

### Frontend
- React hooks для состояния ✅
- Lucide icons для иконок ✅
- React Hot Toast для уведомлений ✅
- Существующий API service ✅

## Временные оценки

- **Backend разработка**: ~3 часа
- **Frontend разработка**: ~3 часа
- **Тестирование**: ~1 час
- **Документация**: ~30 минут
- **Итого**: ~7.5 часов

## Следующие шаги

1. ✅ Создать этот план (DONE)
2. ⏳ **Ожидание подтверждения от пользователя**
3. ⏳ Начать реализацию с Этапа 1
4. ⏳ Тестирование на каждом этапе
5. ⏳ Code review и рефакторинг
6. ⏳ Деплой и мониторинг

---

**Дата создания**: 2025-01-XX
**Статус**: Ожидание подтверждения
**Автор**: AI Assistant

