# Parser Improvement Roadmap

## 1. RSS / JSON Feeds (High Priority)

**Goal:** Обеспечить надёжный источник данных с минимальным парсингом HTML.

- **Что сделать:**
  - Автообнаружение `<link rel="alternate" type="application/rss+xml">`, `/feed`, `/rss`, `wp-json/wp/v2/posts`, Ghost API (`/ghost/api/content/posts/`).
  - Реализовать нормализатор RSS/Atom/JSON-лент: заголовок, ссылка, описание, дата, теги, авторы.
  - Добавить fallback: если лента не доступна, логировать и переходить к HTML.

- **Контекст:**
  - Сократит зависимость от структуры верстки.
  - Покрывает большинство WordPress, Ghost, Substack, Medium и пр.
  - Используется во всех сценариях (cron, `/companies/scan`).

- **Ограничения:**
  - Соблюдать таймауты (≤ 10 c).
  - Не ломать текущий HTML-парсер (работать как параллельный источник).

## 2. JSON-LD и Структурированные Данные (High Priority)

**Goal:** Извлекать точные метаданные даже при слабой верстке.

- **Что сделать:**
  - Сканировать `<script type="application/ld+json">` и искать `@type: NewsArticle / BlogPosting`.
  - Выгружать URL, заголовок, `description`, `datePublished`, `category`, автора и теги.
  - Объединять с уже найденными статьями (из RSS/HTML) по URL/guid.

- **Контекст:**
  - Практически “официальный” источник данных для SEO; OpenAI и крупные компании используют JSON-LD.
  - Нужен как fallback, если RSS/HTML не сработал.
  - В `/companies/scan` помогает строить точный предпросмотр.

- **Ограничения:**
  - Поддержать массивы/сложные структуры (`graph`).
  - Защититься от некорректного JSON (try/except).

## 3. CMS / SPA Adapters (High Priority)

**Goal:** Обрабатывать популярные платформы “из коробки”.

- **Что сделать:**
  - Определить особенности Ghost, Medium, HubSpot, Webflow, Netlify CMS, Next.js.
  - Для каждой платформы описать:
    - как распознать (заголовки, HTML-комментарии, метатеги, сигнатуры JS);
    - как получить список статей (API, JSON в `<script>`, специальные селекторы).
  - Добавить стратегию автоматического подключения адаптера.

- **Контекст:**
  - Позволит не писать кастомные костыли под каждого клиента.
  - Важно для крупных Next.js/Ghost сайтов (OpenAI, Anthropic).
  - Поддержка в cron и в `/companies/scan`.

- **Ограничения:**
  - Адаптеры должны иметь ограничение по времени и количеству запросов.
  - Обработать переопределение базовых стратегий (риски конфликта с общим HTML-парсером).

## 4. Pagination & Category Crawling (Medium Priority)

**Goal:** Собирать больше статей, включая категорийные страницы.

- **Что сделать:**
  - Распознавать `rel="next"` (или кнопки “Load more”) и ограничивать глубину (например, max 5 страниц).
  - На главной странице блога собирать ссылки на категории (`/news/company-announcements`, `/news/research`).
  - Добавить очередь посещения категорий с защитой от циклов (хранить visited set).
  - Сохранять происхождение (какая категория) для аналитики.

- **Контекст:**
  - Важно для сайтов вроде `https://openai.com/news/`, donde каждая категория — отдельная лента.
  - Даст более полный coverage в cron и модальном сканировании.

- **Ограничения:**
  - Контролировать общее число запросов (например, 10 страниц максимум).
  - Не допускать бесконечного обхода (слежение за visited URL + лимиты).

## 5. Deduplication & Update Logic (Medium Priority)

**Goal:** Исключить дубли и корректно обновлять существующие записи.

- **Что сделать:**
  - Использовать `canonical`, `og:url`, RSS `guid`, JSON-LD `url` для идентификации статьи.
  - Проверять хэш контента/заголовка перед добавлением.
  - Если статья уже есть, обновлять описание/категории/теги.
  - Логировать кол-во пропущенных/обновлённых элементов.

- **Контекст:**
  - Исключает дубли при повторном сканировании и при разных источниках (RSS + HTML).
  - Важно для базы новостей (902+ записей).

- **Ограничения:**
  - Не замедлять операции в БД (использовать индексы).
  - Учесть, что некоторые источники меняют URL при обновлении (поддержка soft-duplicates).

## 6. Metadata Enrichment (Medium Priority)

**Goal:** Дополнить карточки новостей полезными данными.

- **Что сделать:**
  - Извлекать `og:title`, `og:description`, `article:published_time`, `meta[name=author]`, теги.
  - Для сайтов с собственными категориями хранить оригинальное значение (например, “Company Announcements”).
  - В API отдавать как дополнительную структуру (не подменяя внутренних `NewsCategory`).

- **Контекст:**
  - Улучшает отображение на UI и качество рекомендаций.
  - Данные можно использовать для фильтров / аналитики.

- **Ограничения:**
  - Следить за длинами полей (не > 500 символов).
  - Не ломать текущие схемы (`NewsItem` и сериализацию).

## 7. API / GraphQL Detection (Optional High Impact)

**Goal:** Если сайт предоставляет официальный API, использовать его.

- **Что сделать:**
  - При загрузке страницы искать ссылки типа `/_next/data/`, `window.__APOLLO_STATE__`, `__NUXT__`, XHR-запросы.
  - Если доступен публичный JSON endpoint, использовать его вместо HTML.
  - Кэшировать и переиспользовать URL API для последующих сканов.

- **Контекст:**
  - Ускоряет и стабилизирует сбор данных (меньше зависимость от верстки).
  - У OpenAI и Anthropic есть internal API — стоит проверить.

- **Ограничения:**
  - Не слать лишние запросы (определять API один раз).
  - Учитывать авторизацию/CSRF, если endpoint закрыт.

## 8. Testing & Tooling (Medium Priority)

**Goal:** Закрепить новые фичи тестами.

- **Что сделать:**
  - Создать фикстуры (сохраненные HTML/RSS/JSON) для ключевых сайтов.
  - Написать unit/integration тесты на `UniversalBlogScraper`.
  - Проверять, что парсер возвращает ожидаемые ссылки, категории, даты.
  - Добавить CLI/скрипт для локальной проверки конкретного сайта (debug).

- **Контекст:**
  - Упростит поддержку и проверки при обновлениях.
  - Позволяет быстрее валидировать новые адаптеры.

- **Ограничения:**
  - Обновлять фикстуры при изменении структуры сайта.
  - Тесты не должны зависеть от сети (используем mock данных).

## 9. Logging & Metrics (Medium Priority)

**Goal:** Видеть эффективность парсинга и аномалии.

- **Что сделать:**
  - В логах фиксировать, какая стратегия сработала (RSS/HTML/JSON-LD/API).
  - Собирать счётчики (сценарий `scrape_all_companies`, `/companies/scan`).
  - Подготовить форматы для внутренних алертов (сколько компаний пустые, доля ошибок, время исполнения).

- **Контекст:**
  - Helps support/debugging, показывает ROI от улучшений.

- **Ограничения:**
  - Не захламлять логи (использовать уровни и поля).
  - Опционально подключить Prometheus/StatsD позже.

---

## Suggested Implementation Order

1. RSS/JSON Feeds + JSON-LD parsing.
2. CMS/SPAs adapters (Ghost, Next.js, HubSpot, Webflow).
3. Pagination + categories.
4. Deduplication + metadata enrichment.
5. API detection + tooling/tests.
6. Logging/metrics enhancements.

